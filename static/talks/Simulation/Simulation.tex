\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Simulation for approximate nearest neighbors using Swissroll dataset},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Simulation for approximate nearest neighbors using Swissroll dataset}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{MBusSchool}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{AACSB}~~~
        \includegraphics[height=0.7cm]{EQUIS}~~~
        \includegraphics[height=0.7cm]{AMBA}
        \end{textblock}
        \vspace*{2cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}


\nojel

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}


\nocover
\blind



\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf\@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage[utf8]{inputenc}
\usepackage[flushleft]{threeparttable}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}


\begin{document}
\maketitle

\hypertarget{notation}{%
\section{Notation}\label{notation}}

The data points are denoted as \(x_i, i=1,\dots,N\) in high-dimensional space \(\mathbb{R}^D\) and \(y_i, i=1,\dots,N\) in low-dimensional space \(\mathbb{R}^d\). While \(\delta_{ij}\) and \(d_{ij}\) denote the distance from \(x_i\) to \(x_j\) and the distance from \(y_i\) to \(y_j\) respectively.

Based on the distances, the ranks of the points \(x\) in high-dimensional space can be calculated as \(\rho_{ij} =|\left\{k: \delta_{i k}<\delta_{i j} \text { or }\left(\delta_{i k}=\delta_{i j} \text { and } k<j\right)\right\} |\), where \(|\cdot|\) denotes the set cardinality. And the ranks of \(y\) are denoted as \(r_{ij} = |\left\{k: d{i k}<d{i j} \text { or }\left(d{i k}=d{i j} \text { and } k<j\right)\right\} |\).

Then the \(K\)-ary neighborhoods of \(x_i\) and \(y_i\) can be denoted as \(U_K(i) = {j: 1 \leq \rho_{ij} \leq K}\) and \(V_K(i) = {j: 1 \leq r_{ij} \leq K}\), respectively, where \(K\) is the size of the neighborhood.

The co-ranking matrix \autocite{Lee2008-sx} can then be defined as
\[
\mathbf{Q}=\left[Q_{k l}\right]_{1 \leqslant k, l \leqslant N-1}, \ \text{with} \ \  Q_{k l}=|\left\{(i, j): \rho_{i j}=k \text { and } r_{i j}=l\right\}|.
\]

\hypertarget{swissroll-dataset}{%
\section{Swissroll dataset}\label{swissroll-dataset}}

To demonstrate, we now apply manifold learning algorithms to the widely-used Swissroll dataset. The key idea to build the Swissroll mapping is to randomly generate \(N\) data points in two-dimensional space, called meta data, and then map them to a three-dimensional space with specific smooth functions and some error terms, denoted as \(X\). The manfold learning algorithms are then applied to the 3-D data \(X\) to get a 2-D embedding \(Y\). In this way, the algorithms can be applied to obtain the embedding \(Y\), and compare it with the meta data.

Suppose a manifold is given by
\[
X = \mathcal{M}(\theta) + \varepsilon,
\]
where \(\mathcal{M}(\theta)\) is the parameterization of the manifold and \(\varepsilon \sim \mathcal{N}(\mu, \ \sigma^{2})\) is the error term.

The mapping function for Swissroll data is given by
\[
\left\{ 
\begin{array}{lcl}
X_1 = \theta_1 \times \cos{\theta_1} + \varepsilon_1, \\
X_2 = \theta_2 + \varepsilon_2, \\
X_3 = \theta_1 \times \sin{\theta_1} + \varepsilon_3.
\end{array}
\right.
\]

Here we consider a Swissroll dataset consists of \texttt{N=2000} points, and the error term \(\varepsilon \sim \mathcal{N}(0, \ 0.05^{2})\). The 3-D plot of Swissroll data and the corresponding meta data are shown in Figure \ref{fig:swissroll}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{Simulation_files/figure-latex/sr-1} 

}

\caption{The 3-D plot and the corresponding meta data plot of a Swissroll data with N=2000.}\label{fig:sr}
\end{figure}

\hypertarget{isomap-embedding}{%
\section{Isomap embedding}\label{isomap-embedding}}

Isomap, short for isometric feature mapping, was one of the first algorithms introduced for manifold learning.
It can be viewed as an extension to MDS, a classical method for embedding dissimilarity information into Euclidean space. Isomap consists of three main steps:

1). Construct the \(K\)-nearest neighborhood graph for the high-dimensional dataset;

2). Estimate the geodesic distances (distances along a manifold) between points in the input using shortest-path distances (Dijkstra's or Floyd's, \textcite{Dijkstra1959-ml}; \textcite{FloydRobert1962-au}) on the neighborhood graph;

3). Use MDS to find points in low-dimensional Euclidean space whose interpoint distances match the distances found in Step 2.

When applying Isomap to the Swissroll data, we initialize the number of nearest neighbors (NN) as \texttt{K=50}, and the number of embedded dimensions as \texttt{d=2}. The 2-D embedding plot is show in Figure \ref{fig:mapping} (b).

\hypertarget{isomap-with-approximate-nearest-neighbors-ann}{%
\section{Isomap with approximate nearest neighbors (ANN)}\label{isomap-with-approximate-nearest-neighbors-ann}}

The first step of Isomap is to assign neighbors to each data point, which requires calculating pairwise distances. The complexity of pairwise distances is \(O(N^2)\) for \(N\) observations, which is not efficient when the data size \(N\) is large.

One solution for large data size is finding approximate nearest neighbors using kd-trees. By constructing kd-trees, there is no need to compute all pairwise distances, and it scales the computation to \(O(N\log(N))\).

When using kd-trees, the dedault value for the error bound parameter is \texttt{eps=0}, which implies exact nearest neighbour search. By tuning this parameter, we could attain the embedding using ANN. We start with \(eps=1\) and get the embedding plot in Figure \ref{fig:mapping} (c).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{Simulation_files/figure-latex/mapping-1} 

}

\caption{The Swissroll meta data and the Isomap 2-D embedding using exact NN and ANN with K=50 and eps=1. (a) Swissroll meta data with N=2000 points; (b) Isomap embedding of Swissroll data using exact NN; (c) Isomap embedding of Swissroll data using ANN.}\label{fig:mapping}
\end{figure}

\hypertarget{quality-measures}{%
\section{Quality measures}\label{quality-measures}}

With various manifold learning algorithms, it is worth considering the question of quality assessment to compare and choose from these methods. We can also apply these measures to explore different variation of each method, including using exact NN and ANN. \textcite{Lee2008-cx} and \textcite{Gracia2014-ae} provided some good reviews on the rank-based criteria of dimensionality reduction quality.

Manifold learning aims at a mapping \(\mathcal{M}\) from high-dimensional datasets to low-dimensional representations such that \(\mathrm{y} = \mathcal{M}(\mathrm{x})\). If we could obtain the inverse \(\mathcal{M}^{-1}\) in closed form, then the reconstruction error can be used as a quality criterion among and within different methods. The reconstruction error can be written as an expectation
\[
E_{\mathrm{rec}}=\mathrm{E}\left[ \left(x-\mathcal{M}^{-1}(\mathcal{M}(x))\right)^{2} \right].
\]
This approach is only applicable to few algorithms, such as PCA and auto-encoders, bacause most ML methods are nonparametric and the closed form of \(\mathcal{M}\) and \(\mathcal{M}^{-1}\) is not available.

Since most methods aim at optimizing a given objective function, it is straightforward to calculate the value of objective function after convergence. However, due to different settings of the function, this comparison is limited within the same algorithm.

Then, we try to assess the intrinsic goal of preserving the data set structure, which can be relaxed in the constrains as in the objective function. For a broader applicability, several quality assessment criteria have been proposed, including three rank-based criteria: the trustworthiness and continuity (T\&C) measures \autocite{Venna2006-nd}, the local continuity meta-criterion (LCMC) \autocite{Chen2009-su}, and the mean relative rank errors (MRREs) \autocite{Lee2007-wq}, and Procrustes measure \autocite{Goldberg2009-tb}. All these criteria are built upon the idea that a faithful embedding preserves the local neighborhood structure of each point \autocite*{Goldberg2009-tb}.
Therefore, they determine the quality of the embedding by analyzing the degree in preserving the K-ary neighborhood structures, for varying value of K.

\hypertarget{trustworthiness-continuity-tc}{%
\subsection{Trustworthiness \& Continuity (T\&C)}\label{trustworthiness-continuity-tc}}

\textcite{Venna2006-nd} defined two quality measures for manifold embeddings to distinguish two type of errors where distant points become neighbors, or neighbors are embedded faraway from each other.

\begin{itemize}
\item
  the trustworthiness of the embedding, where trustworthiness errors are defined as distant input points that entered the same output neighborhood.
  \[M_{T}(K)=1-\frac{2}{G_{K}} \sum_{i=1}^{N} \sum_{j \in V_{K}(i) \notin U_{K}(i)}(\rho{ij}-K),\]
  where the normalizing factor
  \[
  G_{K}=\left\{\begin{array}{ll}
  N K(2 N-3 K-1) & \text { if } K<N / 2 \\
  N(N-K)(N-K-1) & \text { if } K \geqslant N / 2
  \end{array}\right.
  \]
\item
  the continuity of the embedding, where continuity errors are defined as data points in the same input neighborhood but in different output neighborhood.
  \[M_{C}(K)=1-\frac{2}{G_{K}} \sum_{i=1}^{N} \sum_{j \in U_{K}(i) \notin V_{K}(i)}(r{ij}-K).\]
\end{itemize}

\hypertarget{mean-relative-rank-errors-mrres}{%
\subsection{Mean Relative Rank Errors (MRREs)}\label{mean-relative-rank-errors-mrres}}

\textcite{Lee2008-cx} developed the mean relative rank errors which is based on similar principle to that of the T\&C, while the two elements are defined as
\[
\begin{aligned}
&W_{n}(K)=1-\frac{1}{H_{K}} \sum_{i=1}^{N} \sum_{j \in U_{K}(i)} \frac{|\rho{ij}-r{ij}|}{\rho{ij}}, \\
&W_{\nu}(K)=1-\frac{1}{H_{k}} \sum_{i=1}^{n} \sum_{j \in V_{k(i)}} \frac{|\rho{ij}-r{ij}|}{r{ij}},
\end{aligned}
\]
where \(H_K\) is the normalizing factor defined as
\[
H_{K}=n \sum_{i=1}^{K} \frac{|N-2 i+1|}{i}.
\]
The MRREs criterion \(Q_M\) evaluates (using an error value) the first K rows and columns of the co-ranking matrix \(\mathrm{Q}\).

\hypertarget{local-continuity-meta-criterion-lcmc}{%
\subsection{Local Continuity Meta-Criterion (LCMC)}\label{local-continuity-meta-criterion-lcmc}}

\textcite{Chen2009-su} suggested the local continuity criterion to compute the average size of the overlap of K-nearest neighborhoods in the low-dimensional embedding and in the high-dimensional space. The LCMC is defined as
\[
Q_{LC}(K)=1-\frac{1}{N K} \sum_{i=1}^{N}\left| U_K(i) \bigcap V_K(i) \right|-\frac{K^{2}}{N-1}.
\]

If the overlap between two \(K\) neighboring sets is calculated, then the \(Q_{LC}(K)\) gives a general measurement for the local faithfulness of the computed embeddings. The interval of \(Q_{LC}(K)\) is \([0,1]\), and values next to 1 mean a high neighborhood overlap between the two dimensional spaces, and next to 0 values the opposite.

From an intuitive point of view, T\&C and MRREs try to detect what goes wrong in a given embedding, whereas the LCMC accounts for things that work well.

\hypertarget{co-ranking-matrix-q_nxk}{%
\subsection{\texorpdfstring{Co-ranking Matrix (\(Q_{NX}(K)\))}{Co-ranking Matrix (Q\_\{NX\}(K))}}\label{co-ranking-matrix-q_nxk}}

Many different concepts and quality criteria for DR can be summarized using the co-ranking framework, presented by \textcite{Lee2008-cx}. Several of the aforementioned methods based on distance ranking in local neighborhoods (T\&C, MRREs, LCMC), are easily unified into an overall framework.

The co-ranking matrix \(\mathrm{Q}\) is defined with its element being
\[
Q_{kl}=\left|\left\{(i, j) | \rho_{i j}=k \text { and } r_{i j}=l\right\}\right|.
\]

Errors of a DR mapping correspond to off-diagonal entries of this co-ranking matrix.
A point \(j\) that gets a lower rank with respect to a point \(i\) in the low-dimensional space than in the
high-dimensional space, i.e.~\(\rho_{ij} > r_{ij}\), is called an \emph{intrusion}. Analogously, if \(\delta_j\) has a higher rank in the low-dimensional space it is called an \emph{extrusion}. As shown in Figure \ref{fig:K-ins} from \textcite{Lee2008-cx}, intrusions and extrusions correspond to off-diagonal entries in the upper or lower triangle, respectively.

We can also associate the above quality measures with co-ranking matrix using the idea of intrusions and extrusions as Figure \ref{fig:matrix}.

\begin{center}\includegraphics[width=1\linewidth]{../coRanking3} \end{center}

Since the preservation of local relationships is important, rank errors for large ranks are not as critical as rank errors of close points.
Therefore, Lee and Verleyssen distinguished two types of intrusions/extrusions, those within a \(K\)-neighborhood, which are benevolent, and those moving across this boundary, which are malign with respect to quality.
A \emph{K-intrusion} (resp. \emph{K-extrusion}) is an intrusion for which \(r_{ij} < K\) (resp. \(\rho_{ij} < K\)).
Subsequently, mild K-intrusions are events for which \(r_{ij} < \rho_{ij} \leq K\), while hard K-intrusions are defined by \(r_{ij} \leq K < \rho_{ij}\). Mild K-extrusions and hard K-extrusions are defined accordingly.

\begin{center}\includegraphics[width=1\linewidth]{../coRanking2} \end{center}

Define \(Q_{NX}(K)\) as the criterion that summarizes co-ranking matrix \(\mathrm{Q}\) in a simple way: it counts the number of points that remain inside the K-neighborhood while projecting, i.e., all points which keep their rank, and all mild in- and extrusions. It is defined as
\[
Q_{NX}(K)=\frac{1}{K N} \sum_{k=1}^{K} \sum_{l=1}^{K} Q_{k l}.
\]
The range is \(Q_{N X}(K) \in[0,1]\), where 1 means a perfect embedding.

\hypertarget{procrustes-measure-rxy}{%
\subsection{\texorpdfstring{Procrustes measure (\(R(X,Y)\))}{Procrustes measure (R(X,Y))}}\label{procrustes-measure-rxy}}

Despite the \(K\)-ary neighborhood based measures, \textcite{Goldberg2009-tb} also presented the Procrustes measure, a novel measure based on Procrustes rotation that enables quantitative comparison of the output of manifold learning algorithms. The function based on the Procrustes analysis works as a measure of local embedding quality, and compares each neighborhood in the highdimensional space and its corresponding low-dimensional embedding.

First, define the Procrustes statistic \(G(X, Y)\) as
\[
\begin{aligned}
G(X, Y) &= \inf _{\left\{A, b: A^{\prime} A=I, b \in \mathbb{R}^{D}\right\}} \sum_{i=1}^{k}\left\|x_{i}-A y_{i}-b\right\|^{2} \\
&= \inf _{\left\{A, b: A^{\prime} A=I, b \in \mathbb{R}^{D}\right\}} \operatorname{tr}\left(\left(X-Y A^{\prime}-1 b^{\prime}\right)^{\prime}\left(X-Y A^{\prime}-1 b^{\prime}\right)\right) \\
&= \inf _{\left\{A: A^{\prime} A=I\right\}} \operatorname{tr}\left(\left(X-Y A^{\prime}\right)^{\prime} H\left(X-Y A^{\prime}\right)\right) \\
&= \inf _{\left\{A: A^{\prime} A=I\right\}}\left\|H\left(X-Y A^{\prime}\right)\right\|_{F}^{2},
\end{aligned}
\]
where \(\left\| \right\|_{F}\) is the Frobenius norm.

They define how well an embedding preserves the local neighborhoods using the Procrustes statistic \(G(X_i,Y_i)\) of each neighborhood-embedding pair \((X_i,Y_i)\). Therefore, a global embedding that preserves
the local structure can be found by minimizing the sum of the Procrustes statistics of all neighborhood-embedding pairs.

Let \(X_{i}\) be the neighborhood of \(x_{i}(i=1, \ldots, N)\) in the high dimension and \(Y_{i}\) be its embedding. Define
\[
R(X, Y)=\frac{1}{N} \sum_{i=1}^{N} G \left(X_{i}, Y_{i}\right).
\]

The function \(R\) measures the average quality of the neighborhood embeddings. Embedding \(Y\) is considered better than embedding \(\tilde{Y}\) in the local-neighborhood-preserving sense if \(R(X, Y) < R(X, \tilde{Y})\). This means that on the average, \(Y\) preserves the structure of the local neighborhoods better than \(\tilde{Y}\).

\hypertarget{quality-measures-for-isomap-embeddings}{%
\section{Quality measures for Isomap embeddings}\label{quality-measures-for-isomap-embeddings}}

With these quality measures, we can further calculate the Isomap embedding quality of both exact NN and ANN as shown in Table \ref{tab:quality}.

\hypertarget{improvements}{%
\section{Improvements}\label{improvements}}

The difference of the quality measures between exacct NN and ANN is not obvious. To improve the results, there are several tuning parameters.

\hypertarget{optimal-parameters-k-eps}{%
\subsection{Optimal parameters: K, eps}\label{optimal-parameters-k-eps}}

By varying the values of \texttt{K} and \texttt{eps}, the quality measures can guide in finding the optimal parameters. The optimal value for K can be obtained from the quality output, which is NA. Then we focus on tuning \texttt{eps}.

\begin{verbatim}
## eps = 0 
## eps = 0.1 
## eps = 0.2 
## eps = 0.3 
## eps = 0.4 
## eps = 0.5 
## eps = 0.6 
## eps = 0.7 
## eps = 0.8 
## eps = 0.9 
## eps = 1 
## eps = 1.1 
## eps = 1.2 
## eps = 1.3 
## eps = 1.4 
## eps = 1.5 
## eps = 1.6 
## eps = 1.7 
## eps = 1.8 
## eps = 1.9 
## eps = 2
\end{verbatim}

Finally, we can plot all the quality measures and computation time to find optimal \texttt{eps}.

\includegraphics{Simulation_files/figure-latex/eps_plot-1.pdf}

\hypertarget{larger-data-size-n}{%
\subsection{Larger data size: N}\label{larger-data-size-n}}

A larger data size might show an obvious improvement with ANN.

\hypertarget{more-algorithms}{%
\subsection{More algorithms}\label{more-algorithms}}

\hypertarget{more-manifolds}{%
\subsection{More manifolds}\label{more-manifolds}}

\hypertarget{questions}{%
\section{Questions}\label{questions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  With each setting (different algorithms or different NN searching method), we can get
\end{enumerate}

\begin{itemize}
\item
  the embedding plot vs the meta data plot,
\item
  different quality measures.
\end{itemize}

How do we compare the embedding plots?

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Our expectation is to get ``faster and not too much worse'' ANN. ``Faster'' can be achived by increasing the data size and compare the computation time in seconds (``Time\_sec'' column in the quality table).
\end{enumerate}

How do we define ``not too much worse'' with the plots and quality table?

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  To find optimal parameters, how to define ``optimal'' here?
\end{enumerate}

Further, how do we reach a trade-off between computation time and quality measures?

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\item
  Since Isomap is not doing a good job here, what else algorithms would be the top options?
\item
  Similar to 4), what else manifolds despite Swissroll?
\end{enumerate}

\includegraphics{Simulation_files/figure-latex/manifolds-1.pdf}

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}

\noindent

\printbibliography

\end{document}
